{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬의 문자열 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='images/string1.jpg' width=500 height=400>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site = 'web-is-free'\n",
    "print(site.split('-'))\n",
    "site = [ \"web\", \"is\", \"free\" ]\n",
    "print(\"-\".join(site))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "word = \"JAVA   가나다 javascript Aa 가나다 AAaAaA123 %^&* 파이썬\"\n",
    "print(re.sub(\"A\", \"\", word))\n",
    "print(re.sub(\"a\", \"\", word)) \n",
    "print(re.sub(\"Aa\", \"\", word)) \n",
    "print(re.sub(\"(Aa){2}\", \"\", word))\n",
    "print(re.sub(\"[Aa]\", \"\", word)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(\"[가-힣]\", \"\", word))\n",
    "print(re.sub(\"[^가-힣]\", \"\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(\"[&^%*]\", \"\", word))\n",
    "print(re.sub(\"[^가-힣A-Za-z0-9\\s]\", \"\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(\"[\\w\\s]\", \"\", word))\n",
    "print(re.sub(\"\\s\", \"\", word))\n",
    "print(re.sub(\"\\d\", \"\", word))\n",
    "print(re.sub(\"\\D\", \"\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(re.sub(\"[^\\w]\", \"\", word))\n",
    "print(re.sub(\"\\W\", \"\", word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word = re.sub(\"[^가-힣\\s]\", \"\", word)\n",
    "print(new_word)\n",
    "new_word = re.sub(\"\\s+\", \" \", new_word)\n",
    "print(new_word)\n",
    "print(new_word.strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 파이썬으로 구현하는 텍스트 분석(자연어 처리)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. koNLPy를 활용한 형태소 분석\n",
    "### 2. 워드 클라우드\n",
    "### 3. 한국어 기반의 자연어 처리 모듈 - nltk\n",
    "### 4. 텍스트 전처리\n",
    "### 5. 카운트 기반의 단어 표현\n",
    "### 6. 한글 자모 분해와 결합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KoNLPy : 한국어 정보처리를 위한 파이썬 패키지 (https://konlpy.org/ko/latest/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. koNLPy를 활용한 형태소 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Kkma\n",
    "from konlpy.utils import pprint\n",
    "import pandas as pd\n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(kkma.sentences('네, 안녕하세요. 반갑습니다.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(kkma.nouns('질문이나 건의사항은 깃헙 이슈 트래커에 남겨주세요.'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(kkma.pos('오류보고는 실행환경, 에러메세지와함께 설명을 최대한상세히!^^'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [형태소 분석기 비교]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = '이것은 형태소 분석기 입니다 아버지가방에들어가신다'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum  \n",
    "hannanum = Hannanum() \n",
    "pprint(hannanum.nouns(sample))\n",
    "pprint(hannanum.morphs(sample))\n",
    "pprint(hannanum.pos(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma = Kkma() \n",
    "pprint(kkma.nouns(sample))\n",
    "pprint(kkma.morphs(sample))\n",
    "pprint(kkma.pos(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt                                 \n",
    "okt = Okt()\n",
    "pprint(okt.nouns(sample))\n",
    "pprint(okt.morphs(sample))\n",
    "pprint(okt.pos(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Komoran                    \n",
    "komoran = Komoran()\n",
    "pprint(komoran.nouns(sample))\n",
    "pprint(komoran.morphs(sample))\n",
    "pprint(komoran.pos(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hannanum.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kkma.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "komoran.tagset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagsets = pd.DataFrame()                            ## 빈 데이터프레임을 만든다. \n",
    "N = 67\n",
    "                                                   ##  한글 형태소 분석기에 있는 품사에 대한 정보를 데이터프레임에 넣는다. \n",
    "tagsets[\"Hannanum-기호\"] = list(hannanum.tagset.keys()) + list(\"*\" * (N - len(hannanum.tagset)))\n",
    "tagsets[\"Hannanum-품사\"] = list(hannanum.tagset.values()) + list(\"*\" * (N - len(hannanum.tagset)))\n",
    "tagsets[\"Kkma-기호\"] = list(kkma.tagset.keys()) + list(\"*\" * (N - len(kkma.tagset)))\n",
    "tagsets[\"Kkma-품사\"] = list(kkma.tagset.values()) + list(\"*\" * (N - len(kkma.tagset)))\n",
    "tagsets[\"Komoran-기호\"] = list(komoran.tagset.keys()) + list(\"*\" * (N - len(komoran.tagset)))\n",
    "tagsets[\"Komoran-품사\"] = list(komoran.tagset.values()) + list(\"*\" * (N - len(komoran.tagset)))\n",
    "tagsets[\"OKT-기호\"] = list(okt.tagset.keys()) + list(\"*\" * (N - len(okt.tagset)))\n",
    "tagsets[\"OKT-품사\"] = list(okt.tagset.values()) + list(\"*\" * (N - len(okt.tagset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(tagsets.head(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 워드 클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_path = \"data/THEdog.ttf\"   #폰트파일의 위치\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud            ## 워드 클라우드 모듈을 사용한다 \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "myfontpath = \"data/THEdog.ttf\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(                        ## 워드클라우드 객체를 만들때 한글로 출력되도록 객체를 만든다 \n",
    "    font_path = myfontpath,\n",
    "    width = 200,\n",
    "    height = 200\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"둘리 도우너 또치 마이콜 희동이 둘리 둘리 도우너 또치 토토로 둘리 올라프 토토로 올라프 올라프\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wc.generate(text)   \n",
    "wc.to_file('output/ptest2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.imshow(wc, interpolation='bilinear')               ## 워드 클라우드 이미지로 출력한다 \n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(\n",
    "    font_path = myfontpath,\n",
    "    background_color='white',                     ## 배경색을 지정한다 \n",
    "    width = 800,\n",
    "    height = 800\n",
    ")\n",
    "wc = wc.generate(text)\n",
    "fig = plt.figure()\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = {'파이썬':7, '넘파이':3, '판다스':5, '매트플롭립':2, '시본':2, '폴리엄':2}             ## 특정 단어의 빈도를 딕셔너리로 만든다 \n",
    "\n",
    "wc = wc.generate_from_frequencies(keywords)        ## 빈도별로 워드클라우드를 만든다 \n",
    "\n",
    "fig = plt.figure()\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image                                ## 이미지 파일을 처리하는 모듈을 사용한다. \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2d2_mask = np.array(Image.open('data/r2d2.JPG'))       ## 이미지를 읽어와서 다차원 배열로 변환한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import STOPWORDS  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set()                               ## 한글은 별도로 집합으로 불용어를 만든다 \n",
    "stopwords.add(\"은\")\n",
    "stopwords.add(\"입니다\")\n",
    "stopwords.add(\"것인가\")\n",
    "stopwords.add(\"처럼\")\n",
    "\n",
    "wc = WordCloud( stopwords=stopwords,              ## 워드 클라우드 객체를 만든다 \n",
    "                          font_path = myfontpath,\n",
    "                          background_color='white',\n",
    "                           width = 800,\n",
    "                           height = 800,\n",
    "                          mask=r2d2_mask)            ## 마스크 인자에 이미지를 전달한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = ['로봇 처럼 표시하는 것을 보기 위해 이것 은 예문 입니다 가을이라 겨울 바람 솔솔 불어오니 ',\n",
    "         '여러분 의 문장을 넣 으세요 ㅎㅎㅎ 스타워즈 영화에 나오는 다양한 로봇처럼 r2d2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wc.generate_from_text(texts[0]+texts[1])    ## 두 개의 문자을 연결해서 워드클라우드를 만든다 \n",
    "wc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc.to_file('output/ptest3.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(wc, interpolation=\"bilinear\")         ## 이미지를 출력하면 전달된 모양에 따라 표시한다 \n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 한국어 기반의 자연어 처리 모듈 : nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                     ## 한국어 자연어처리 모듈 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.corpus import kobill\n",
    "files_ko = kobill.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ko = kobill.open('1809898.txt').read()         ## 특정 텍스트 파일을 읽어온다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_ko"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt      \n",
    "t = Okt()\n",
    "tokens_ko = t.nouns(doc_ko)                ## 텍스트에서 명사를 추출한다. \n",
    "print(tokens_ko[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_text = nltk.Text(tokens_ko, name='국군부대의 소말리아 해역 파견연장 동의안')       ## 명사로 추출한 것을 텍스트 객체로 만든다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(nouns_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nouns_text.tokens)                         ##  명사로 분리된 개수를 확인한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(nouns_text.tokens))                   ## 유일한 단어의 개수를 확인한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_text.tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_text.vocab()                    ## 동일한 단어의 발생 빈도를 확인한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_path = \"data/THEdog.ttf\"   #폰트파일의 위치\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "nouns_text.plot(50)                         ## 단어별로 발생빈도에 맞도록 그래프를 그린다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_text.count('파견')                  ## 특정 단어의 발생빈도를 확인한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_text.count('소말리아')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_text.concordance('소말리아')             ## 특정 단어가 있는 곳 리턴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nouns_text.vocab().most_common(150)                 ## 가장 많이 발생한 단어를 선택한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(font_path=myfontpath,                                    ## .한글에 대한 위치를 표시한다. \n",
    "                      relative_scaling = 0.2,\n",
    "                      background_color='white',\n",
    "                      ).generate_from_frequencies(dict(data))             ## .단어별 빈도수를 딕셔너리로 변환해서 전달한다 \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wc)                                                      ## .이미지를 출력한다 \n",
    "plt.axis(\"off\")                                                            ## 그래프에 대한 축을 표시하지 않는다 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(font_path=myfontpath,                                    ## .한글에 대한 위치를 표시한다. \n",
    "                      relative_scaling = 0.2,\n",
    "                      background_color='white',\n",
    "                      colormap = 'coolwarm',\n",
    "                      mask=r2d2_mask\n",
    "                      ).generate_from_frequencies(dict(data))             ## .단어별 빈도수를 딕셔너리로 변환해서 전달한다 \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wc)                                                      ## .이미지를 출력한다 \n",
    "plt.axis(\"off\")                                                            ## 그래프에 대한 축을 표시하지 않는다 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(font_path=myfontpath,                                    ## .한글에 대한 위치를 표시한다. \n",
    "                      relative_scaling = 0.2,\n",
    "                      background_color='white',\n",
    "                      colormap = 'plasma',\n",
    "                      mask=np.array(Image.open('images/clover.jpg'))\n",
    "                      ).generate_from_frequencies(dict(data))             ## .단어별 빈도수를 딕셔너리로 변환해서 전달한다 \n",
    "plt.figure(figsize=(10,6))\n",
    "plt.imshow(wc)                                                      ## .이미지를 출력한다 \n",
    "plt.axis(\"off\")                                                            ## 그래프에 대한 축을 표시하지 않는다 \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "?WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 텍스트 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **한국어 전처리 패키지 : PyKoSpacing & Py-Hanspell**\n",
    "#### 전희원님이 개발한 PyKoSpacing은 한국어 띄어쓰기 패키지로 띄어쓰기가 되어있지 않은 문장을 띄어쓰기를 한 문장으로 변환해주는 패키지이다. PyKoSpacing은 대용량 코퍼스를 학습하여 만들어진 띄어쓰기 딥 러닝 모델로 준수한 성능을 가지고 있다.\n",
    "\n",
    "##### pip install git+https://github.com/haven-jeon/PyKoSpacing.git\n",
    "##### pip install git+https://github.com/ssut/py-hanspell.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = '김철수는 극중 두 인격의 사나이 이광수 역을 맡았다. 철수는 한국 유일의 태권도 전승자를 가리는 결전의 날을 앞두고 10년간 함께 훈련한 사형인 유연재(김광수 분)를 찾으러 속세로 내려온 인물이다.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_sent = sent.replace(\" \", '') # 띄어쓰기가 없는 문장 임의로 만들기\n",
    "print(new_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pykospacing import spacing\n",
    "\n",
    "kospacing_sent = spacing(new_sent)\n",
    "print(sent)\n",
    "print(kospacing_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hanspell import spell_checker\n",
    "\n",
    "sent = \"맞춤법 틀리면 외 않되? 쓰고싶은대로쓰면돼지 \"\n",
    "spelled_sent = spell_checker.check(sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spelled_sent = spell_checker.check(new_sent)\n",
    "\n",
    "hanspell_sent = spelled_sent.checked\n",
    "print(hanspell_sent)\n",
    "print(kospacing_sent) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 카운트 기반의 단어 표현"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화 : 주어진 코퍼스(corpus)에서 토큰(token)이라 불리는 단위로 나누는 작업을 토큰화(tokenization)라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방법(1) - nltk 패키지의 word_tokenize() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "corpus = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"\n",
    "word_tokens1 = word_tokenize(example)\n",
    "print(word_tokens1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방법(2) - Okt 객체의 morphs() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()  \n",
    "corpus = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"  \n",
    "word_tokens2 = t.morphs(corpus)  \n",
    "print(word_token2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든 게 때\"\n",
    "stop_words = stop_words.split(' ')\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens1: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든 게 때\"\n",
    "stop_words = stop_words.split(' ')\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens2: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 방법(3) - Okt 객체의 nouns() 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()  \n",
    "corpus = \"고기를 아무렇게나 구우려고 하면 안 돼. 고기라고 다 같은 게 아니거든. 예컨대 삼겹살을 구울 때는 중요한 게 있지.\"  \n",
    "word_tokens3 = t.nouns(corpus)  \n",
    "print(word_tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = \"아무거나 아무렇게나 어찌하든지 같다 비슷하다 예컨대 이럴정도로 하면 아니거든 게 때\"\n",
    "stop_words = stop_words.split(' ')\n",
    "\n",
    "result = [] \n",
    "for w in word_tokens3: \n",
    "    if w not in stop_words: \n",
    "        result.append(w) \n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "t = Okt()  \n",
    "corpus = \"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다\"  \n",
    "word_tokens3 = t.morphs(corpus)  \n",
    "print(word_tokens3)\n",
    "nouns_text = nltk.Text(word_tokens3)\n",
    "nouns_text.vocab()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한국어 불용어 참조 사이트 : https://www.ranks.nl/stopwords/korean\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 카운트 기반의 단어 표현\n",
    "\n",
    "### Bag of Words란 단어들의 순서는 고려하지 않고, 단어들의 출현 빈도에만 집중하는 텍스트 데이터의 수치화 표현 방법\n",
    "### DTM(또는 TDM)은 BoW 의 확장으로서 TF 방법과 TF_IDF 방식으로 생성 가능\n",
    "### TF-IDF 는 빈도수 기반 단어 표현에 단어의 중요도에 따른 가중치를 주는 방법 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 사이킷 런에서는 단어의 빈도를 Count하여 Vector로 만드는 CountVectorizer 클래스를 지원"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다\"]\n",
    "vector = CountVectorizer()\n",
    "r = vector.fit_transform(corpus).toarray()\n",
    "print(r) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(r.shape)\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = [\"정부가 발표하는 물가상승률과 소비자가 느끼는 물가상승률은 다르다\"]\n",
    "vector = CountVectorizer(stop_words=[\"정부가\", \"소비자가\"])\n",
    "r = vector.fit_transform(corpus).toarray()\n",
    "print(r) # 코퍼스로부터 각 단어의 빈도 수를 기록한다.\n",
    "print(r.shape)\n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # 데이터프레임 사용을 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "  '먹고 싶은 사과',\n",
    "  '먹고 싶은 바나나',\n",
    "  '길고 노란 바나나 바나나',\n",
    "  '저는 과일이 좋아요'\n",
    "] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()\n",
    "dtm = vector.fit_transform(corpus).toarray()\n",
    "print(dtm) \n",
    "print(vector.vocabulary_) # 각 단어들의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = pd.DataFrame(dtm, columns = vector.get_feature_names())\n",
    "display(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vector = TfidfVectorizer()\n",
    "dtm = vector.fit_transform(corpus).toarray()\n",
    "print(dtm) \n",
    "print(vector.vocabulary_) # 각 단어들의 인덱스가 어떻게 부여되었는지를 보여준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = pd.DataFrame(dtm, columns = vector.get_feature_names())\n",
    "display(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "           \"커피 파스타 치킨 샐러드 아이스크림\",\n",
    "           \"커피 우동 소고기김밥 귤\",\n",
    "           \"참치김밥 커피 오뎅\",\n",
    "           \"샐러드 피자 파스타 콜라\",\n",
    "           \"티라무슈 햄버거 콜라\",\n",
    "           \"파스타 샐러드 커피\"    \n",
    "]\n",
    "vector = CountVectorizer()\n",
    "dtm = vector.fit_transform(corpus).toarray()\n",
    "print(dtm) \n",
    "tf = pd.DataFrame(dtm, columns = vector.get_feature_names())\n",
    "display(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com = dtm.T @ dtm  # 동시 출현횟수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comdf = pd.DataFrame(com, columns = vector.get_feature_names(), index = vector.get_feature_names())\n",
    "display(comdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = \"aaa bbb ccc\"\n",
    "d2 = \"aaa bbb ddd\"\n",
    "d3 = \"aaa bbb ccc\"\n",
    "d4 = \"xxx yyy zzz\"\n",
    "dd = [d1, d2, d3, d4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = CountVectorizer()\n",
    "dtm = vector.fit_transform(dd).toarray()\n",
    "print(dtm) \n",
    "print(vector.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "similarity_simple_pair = cosine_similarity(dtm, dtm)\n",
    "print(similarity_simple_pair)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 한글 자모 분해와 결합"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hgtk                      ## 한글의 자음과 모음을 분리하는 모듈을 사용한다 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 한글 자모 분해, 조합(오토마타), 조사 붙이기, 초/중/종 분해조합, 한글/한자/영문 여부 체크 등을 지원합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.letter.decompose('감')          ## 특정 글자를 분리하면 초성 중성 종성으로 분리된다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.letter.compose('ㄱ', 'ㅏ', 'ㅁ')      ## 분리된 글자를 하나의 글자로 합친다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = '''타밀어는 드라비다어족의 남부 계통, 즉 남부드라비다어파에 속하는 언어이다.\n",
    "공식어로 지정된 인도의 주요 언어 중에서 타밀어와 계통적으로 가장 가까운 것은 말라얄람어인데, \n",
    "9세기 무렵까지 말라얄람어는 타밀어의 방언이었다.\n",
    "이 두 언어 간에는 선사 시대에 일어난 서부 방언(말라얄람어의 원형) 분열의 증거가 되는 많은 차이가 있지만, \n",
    "13~14세기 무렵까지도 두 언어는 완전히 서로 다른 언어로 분리되지 않은 채였다.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = hgtk.text.decompose(sample_text)        ## 여러 문장에 대해 단어를 분리한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.text.compose(s)[:40]                     ## 분리된 것을 하나로 합친다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.checker.is_hangul('한글입니다')           ## 한글 여부를 확인한다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.checker.is_hangul('no한글입니다')         ## 일부 영어가 들어가면 한글로 인식하지 않는다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.checker.is_hangul('it is english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.checker.is_hanja('大韓民國')                  ## 한자도 확인할 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.checker.is_hanja('大한민국')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.checker.is_hanja('대한민국')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Josa\n",
    "EUN_NEUN - 은/는"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.josa.attach('하늘', hgtk.josa.EUN_NEUN)                 ## 단어에 맞는 조사를 붙여볼 수 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.josa.attach('바다', hgtk.josa.EUN_NEUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.josa.attach('하늘', hgtk.josa.I_GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.josa.attach('바다', hgtk.josa.I_GA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.josa.attach('하늘', hgtk.josa.EUL_REUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hgtk.josa.attach('바다', hgtk.josa.EUL_REUL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Okt\n",
    "okt = Okt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '태양계는 지금으로부터 약 46억 년 전, 거대한 분자 구름의 일부분이 중력 붕괴를 일으키면서 형성되었다'\n",
    "\n",
    "okt.pos(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.morphs(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "okt.nouns(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 단어 분석을 위한 패키지 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import font_manager, rc\n",
    "import wordcloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-1. 파일 읽기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'etnews.kr_facebook_2016-01-01_2018-08-01_4차 산업혁명'\n",
    "inputFileName = 'data/'+filename\n",
    "data = json.loads(open(inputFileName+'.json', 'r', encoding='utf-8').read())\n",
    "data #출력하여 내용 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-2. 분석할 데이터 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "message = ''\n",
    "\n",
    "for item in data:\n",
    "    if 'message' in item.keys(): \n",
    "        message = message + re.sub(r'[^\\w]', ' ', item['message']) +''\n",
    "        \n",
    "message #출력하여 내용 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1-3. 품사 태깅 : 명사 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp = Okt()\n",
    "message_N = nlp.nouns(message)\n",
    "message_N   #출력하여 내용 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 데이터 탐색"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2-1. 단어 빈도 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = Counter(message_N)\n",
    "\n",
    "count   #출력하여 내용 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_count = dict()\n",
    "\n",
    "for tag, counts in count.most_common(80):\n",
    "    if(len(str(tag))>1):\n",
    "        word_count[tag] = counts\n",
    "        print(\"%s : %d\" % (tag, counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 히스토그램"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import font_manager, rc\n",
    "font_path = \"data/THEdog.ttf\"   #폰트파일의 위치\n",
    "font_name = font_manager.FontProperties(fname=font_path).get_name()\n",
    "rc('font', family=font_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "plt.xlabel('키워드')\n",
    "plt.ylabel('빈도수')\n",
    "plt.grid(True)\n",
    "\n",
    "sorted_Keys = sorted(word_count, key=word_count.get, reverse=True)\n",
    "sorted_Values = sorted(word_count.values(), reverse=True)\n",
    "\n",
    "plt.bar(range(len(word_count)), sorted_Values, align='center')\n",
    "plt.xticks(range(len(word_count)), list(sorted_Keys), rotation='75')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 워드클라우드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = WordCloud(font_path, background_color='ivory', width=800, height=600)\n",
    "cloud=wc.generate_from_frequencies(word_count)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(cloud)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
