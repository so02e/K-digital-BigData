{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "print(urllib.parse.quote('가나다ABC123'))\n",
    "print(urllib.parse.quote_plus('가나다ABC123'))\n",
    "print(urllib.parse.quote('가나다 ABC123'))\n",
    "print(urllib.parse.quote_plus('가나다 ABC123'))\n",
    "query_str1 = \"q=\"+urllib.parse.quote('가나다')\n",
    "query_str2 = \"q=\"+urllib.parse.quote_plus('가나다')\n",
    "query_str3 = urllib.parse.urlencode({\"q\" : \"가나다\"})\n",
    "print(\"-------------------------------------------\")\n",
    "print(query_str1)\n",
    "print(query_str2)\n",
    "print(query_str3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "busNum = '360'\n",
    "key = '%2BjzsSyNtwmcqxUsGnflvs3rW2oceFvhHR8AFkM3ao%2Fw50hwHXgGyPVutXw04uAXvrkoWgkoScvvhlH7jgD4%2FRQ%3D%3D'\n",
    "url1 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getBusRouteList?serviceKey='+key+'&strSrch='+busNum\n",
    "res = req.urlopen(url1)\n",
    "\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), features=\"xml\")\n",
    "\n",
    "busRouteId = None\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    busRouteId = itemList.find('busRouteId').string\n",
    "    busRouteNm = itemList.find('busRouteNm').string\n",
    "    if busRouteNm == busNum :\n",
    "        break\n",
    "\n",
    "url2 = 'http://ws.bus.go.kr/api/rest/busRouteInfo/getStaionByRoute?ServiceKey='+key+'&busRouteId='+busRouteId\n",
    "res = req.urlopen(url2)\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), features=\"xml\")\n",
    "\n",
    "busPos = []\n",
    "for itemList in soup.find_all('itemList') :\n",
    "    gpsY = itemList.find('gpsY').string\n",
    "    gpsX = itemList.find('gpsX').string\n",
    "\n",
    "    busPos.append((gpsY, gpsX))\n",
    "\n",
    "print('[ ' + busNum + '번 버스의 운행 위치 ]')\n",
    "if len(busPos) != 0 :\n",
    "    print(busNum + '번 버스 ' + str(len(busPos)) + '대 운행중...')\n",
    "    for lat,lng in busPos :\n",
    "        print(lat+','+lng)\n",
    "else :\n",
    "    print('현재 운행중인 ' + busNum + '번 버스가 없어요...')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/LampScpgmtb/'+startIndex+'/'+endIndex+'/'\n",
    "savename = 'output/edu.xml'\n",
    "req.urlretrieve(url, savename)\n",
    "\n",
    "xml = open(savename, 'r', encoding='utf-8').read()\n",
    "soup = BeautifulSoup(xml, 'xml')\n",
    "\n",
    "pjList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    up_nm = itemList.find('UP_NM').string\n",
    "    up_nm = '없음' if up_nm is None else up_nm\n",
    "    pgm_nm = itemList.find('PGM_NM').string\n",
    "    pgm_nm = '없음' if pgm_nm is None else pgm_nm\n",
    "    target_nm = itemList.find('TARGET_NM').string\n",
    "    target_nm = '없음' if target_nm is None else target_nm\n",
    "    u_price = itemList.find('U_PRICE').string\n",
    "    u_price = '없음' if u_price is None else u_price\n",
    "    pjList.append((up_nm, pgm_nm, target_nm, u_price))\n",
    "\n",
    "print('[ 서울 청소년 수련관 강좌 리스트 ]')\n",
    "for up_nm,pgm_nm,target_nm,u_price in pjList :\n",
    "    print(up_nm+','+pgm_nm+','+target_nm+','+str(u_price))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "key = '796143536a756e69313134667752417a'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "date = '20201010'\n",
    "\n",
    "url = 'http://openapi.seoul.go.kr:8088/'+key+'/'+contentType+'/CardSubwayStatsNew/'+startIndex+'/'+endIndex+'/'+date+'/'\n",
    "savename = 'output/subway.xml'\n",
    "req.urlretrieve(url, savename)\n",
    "\n",
    "xml = open(savename, 'r', encoding='utf-8').read()\n",
    "soup = BeautifulSoup(xml, 'xml')\n",
    "\n",
    "subwayList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    line_num = itemList.find('LINE_NUM').string\n",
    "    sub_sta_nm = itemList.find('SUB_STA_NM').string\n",
    "    ride_pasgr_num = itemList.find('RIDE_PASGR_NUM').string\n",
    "    alight_pasgr_num = itemList.find('ALIGHT_PASGR_NUM').string\n",
    "    subwayList.append((line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num))\n",
    "\n",
    "print('[ 서울시 지하철호선별 역별 승하차 인원 정보 ]')\n",
    "for line_num, sub_sta_nm, ride_pasgr_num, alight_pasgr_num in subwayList :\n",
    "    print(line_num+','+sub_sta_nm+','+ride_pasgr_num+','+alight_pasgr_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import io\n",
    "\n",
    "key = '4WQ7X833TXC370SUTDX4'\n",
    "contentType = 'xml'\n",
    "startIndex = '1'\n",
    "endIndex = '100'\n",
    "\n",
    "url = 'http://ecos.bok.or.kr/api/KeyStatisticList/'+key+'/'+contentType+'/kr/'+startIndex+'/'+endIndex+'/'\n",
    "res = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(res.read().decode('utf-8'), 'xml')\n",
    "ecoList = []\n",
    "for itemList in soup.find_all('row') :\n",
    "    class_name = itemList.find('CLASS_NAME').string\n",
    "    class_name = '없음' if class_name is None else class_name\n",
    "    keystat_name = itemList.find('KEYSTAT_NAME').string\n",
    "    keystat_name = '없음' if keystat_name is None else keystat_name\n",
    "    data_value = itemList.find('DATA_VALUE').string\n",
    "    data_value = '없음' if data_value is None else data_value\n",
    "    cycle = itemList.find('CYCLE').string\n",
    "    cycle = '없음' if cycle is None else cycle\n",
    "    unit_name = itemList.find('UNIT_NAME').string\n",
    "    unit_name = '없음' if unit_name is None else unit_name\n",
    "    ecoList.append((class_name, keystat_name, data_value, cycle, unit_name))\n",
    "\n",
    "print('[ 100대 통계 지표 ]')\n",
    "for class_name, keystat_name, data_value, cycle, unit_name in ecoList :\n",
    "    print(class_name+','+keystat_name+','+data_value+','+cycle+','+unit_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = '수능'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/blog.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "rescode = response.getcode()\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 블로그 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "\n",
    "client_key = 'izGsqP2exeThwwEUVU3x'\n",
    "client_secret = 'WrwbQ1l6ZI'\n",
    "query = 'tns'\n",
    "encText = urllib.parse.quote_plus(query)\n",
    "\n",
    "num = 100\n",
    "naver_url = 'https://openapi.naver.com/v1/search/news.json?query=' + encText + '&display=' + str(num)\n",
    "\n",
    "request = urllib.request.Request(naver_url)\n",
    "request.add_header(\"X-Naver-Client-Id\",client_key)\n",
    "request.add_header(\"X-Naver-Client-Secret\",client_secret)\n",
    "response = urllib.request.urlopen(request)\n",
    "\n",
    "rescode = response.getcode()\n",
    "\n",
    "if(rescode == 200):\n",
    "    response_body = response.read()\n",
    "    dataList = json.loads(response_body)\n",
    "    count = 1\n",
    "    print('[' + query + '에 대한 네이버 뉴스 글 ]')\n",
    "    for data in dataList['items'] :\n",
    "        print (str(count) + ' : ' + data['title'])\n",
    "        print ('[' + data['description'] + ']')\n",
    "        count += 1\n",
    "else:\n",
    "    print('오류 코드 : ' + rescode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "\n",
    "api_key = \"RvnZeIl8ra88reu8fm23m0bST\"\n",
    "api_secret = \"wTRylK94GK2KmhZUnqXonDaIszwAsS6VPvpSsIo6EX5GQLtzQo\"\n",
    "access_token = \"959614462004117506-dkWyZaO8Bz3ZXh73rspWfc1sQz0EnDU\"\n",
    "access_token_secret = \"rxDWfg7uz1yXMTDwijz0x90yWhDAnmOM15R6IgC8kmtTe\"\n",
    "\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n",
    "\n",
    "keyword = '치맥'   \n",
    "search = []   \n",
    "\n",
    "cnt = 1\n",
    "while(cnt <= 10):   \n",
    "    tweets = api.search(q=keyword, count=100)\n",
    "    for tweet in tweets :\n",
    "        search.append(tweet)\n",
    "    cnt += 1\n",
    "\n",
    "data = {}   \n",
    "i = 1   \n",
    "print('[' + keyword + '에 대한 트윗 글 ]')    \n",
    "for tweet in search:\n",
    "    data['text'] = tweet.text   \n",
    "    print(i, \" : \", data)   \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import urllib.request\n",
    "import datetime\n",
    "import time\n",
    "import json\n",
    "\n",
    "client_id = '0LHQM4VX_MQM6JfkXofa'\n",
    "client_secret = 'OcPgqpswCg'\n",
    "\n",
    "\n",
    "#[CODE 1]\n",
    "def getRequestUrl(url):    \n",
    "    req = urllib.request.Request(url)\n",
    "    req.add_header(\"X-Naver-Client-Id\", client_id)\n",
    "    req.add_header(\"X-Naver-Client-Secret\", client_secret)\n",
    "    \n",
    "    try: \n",
    "        response = urllib.request.urlopen(req)\n",
    "        if response.getcode() == 200:\n",
    "            print (\"[%s] Url Request Success\" % datetime.datetime.now())\n",
    "            return response.read().decode('utf-8')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print(\"[%s] Error for URL : %s\" % (datetime.datetime.now(), url))\n",
    "        return None\n",
    "\n",
    "#[CODE 2]\n",
    "def getNaverSearch(node, srcText, start, display):    \n",
    "    base = \"https://openapi.naver.com/v1/search\"\n",
    "    node = \"/%s.json\" % node\n",
    "    parameters = \"?query=%s&start=%s&display=%s\" % (urllib.parse.quote(srcText), start, display)\n",
    "    \n",
    "    url = base + node + parameters    \n",
    "    responseDecode = getRequestUrl(url)   #[CODE 1]\n",
    "    \n",
    "    if (responseDecode == None):\n",
    "        return None\n",
    "    else:\n",
    "        return json.loads(responseDecode)\n",
    "\n",
    "#[CODE 3]\n",
    "def getPostData(post, jsonResult, cnt):    \n",
    "    title = post['title']\n",
    "    description = post['description']\n",
    "    org_link = post['originallink']\n",
    "    link = post['link']\n",
    "    \n",
    "    pDate = datetime.datetime.strptime(post['pubDate'],  '%a, %d %b %Y %H:%M:%S +0900')\n",
    "    pDate = pDate.strftime('%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    jsonResult.append({'cnt':cnt, 'title':title, 'description': description, \n",
    "'org_link':org_link,   'link': org_link,   'pDate':pDate})\n",
    "    return    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "검색어를 입력하세요:  황사\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.203895] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.372907] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.552920] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.717933] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:36.890946] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.074960] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.258973] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.448988] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.630001] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "[2021-03-29 12:35:37.818015] Url Request Success\n",
      "%ED%99%A9%EC%82%AC\n",
      "HTTP Error 400: Bad Request\n",
      "[2021-03-29 12:35:37.884021] Error for URL : https://openapi.naver.com/v1/search/news.json?query=%ED%99%A9%EC%82%AC&start=1001&display=100\n",
      "전체 검색 : 195273 건\n",
      "가져온 데이터 : 1000 건\n",
      "output/황사_naver_news.json SAVED\n"
     ]
    }
   ],
   "source": [
    "node = 'news'   # 크롤링 할 대상\n",
    "srcText = input('검색어를 입력하세요: ')\n",
    "cnt = 0\n",
    "jsonResult = []\n",
    "\n",
    "jsonResponse = getNaverSearch(node, srcText, 1, 100)  #[CODE 2]\n",
    "total = jsonResponse['total']\n",
    " \n",
    "while ((jsonResponse != None) and (jsonResponse['display'] != 0)):         \n",
    "    for post in jsonResponse['items']:\n",
    "        cnt += 1\n",
    "        getPostData(post, jsonResult, cnt)  #[CODE 3]       \n",
    "        \n",
    "    start = jsonResponse['start'] + jsonResponse['display']\n",
    "    jsonResponse = getNaverSearch(node, srcText, start, 100)  #[CODE 2]\n",
    "       \n",
    "print('전체 검색 : %d 건' %total)\n",
    "    \n",
    "with open('%s_naver_%s.json' % (srcText, node), 'w', encoding='utf8') as outfile:\n",
    "    jsonFile = json.dumps(jsonResult,  indent=4, sort_keys=True,  ensure_ascii=False)\n",
    "                        \n",
    "    outfile.write(jsonFile)\n",
    "        \n",
    "print(\"가져온 데이터 : %d 건\" %(cnt))\n",
    "print ('output/%s_naver_%s.json SAVED' % (srcText, node))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "#[CODE 1]\n",
    "def hollys_store(result):\n",
    "    for page in range(1,59):\n",
    "        Hollys_url = 'https://www.hollys.co.kr/store/korea/korStore.do?pageNo=%d&sido=&gugun=&store=' %page\n",
    "        print(Hollys_url)\n",
    "        html = urllib.request.urlopen(Hollys_url)\n",
    "        soupHollys = BeautifulSoup(html, 'html.parser')\n",
    "        tag_tbody = soupHollys.find('tbody')\n",
    "        for store in tag_tbody.find_all('tr'):\n",
    "            if len(store) <= 3:\n",
    "                break\n",
    "            store_td = store.find_all('td')\n",
    "            store_name = store_td[1].string\n",
    "            store_sido = store_td[0].string\n",
    "            store_address = store_td[3].string\n",
    "            store_phone = store_td[5].string\n",
    "            result.append([store_name]+[store_sido]+[store_address]\n",
    "                          +[store_phone])\n",
    "    return\n",
    "\n",
    "#[CODE 0]\n",
    "result = []\n",
    "print('Hollys store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "hollys_store(result)   #[CODE 1] 호출 \n",
    "hollys_tbl = pd.DataFrame(result, columns=('store', 'sido-gu', 'address','phone'))\n",
    "hollys_tbl.to_csv('output/hollys.csv', encoding='cp949', mode='w', index=True)\n",
    "del result[:]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import datetime\n",
    "\n",
    "from selenium import webdriver\n",
    "import time\n",
    "\n",
    "#[CODE 1]\n",
    "def CoffeeBean_store(result):\n",
    "    CoffeeBean_URL = \"https://www.coffeebeankorea.com/store/store.asp\"\n",
    "    wd = webdriver.Chrome('c:/Temp/chromedriver.exe')\n",
    "             \n",
    "    for i in range(1, 370):  #매장 수 만큼 반복\n",
    "        wd.get(CoffeeBean_URL)\n",
    "        time.sleep(1)  #웹페이지 연결할 동안 1초 대기\n",
    "        try:\n",
    "            wd.execute_script(\"storePop2(%d)\" %i)\n",
    "            time.sleep(1) #스크립트 실행 할 동안 1초 대기\n",
    "            html = wd.page_source\n",
    "            soupCB = BeautifulSoup(html, 'html.parser')\n",
    "            store_name_h2 = soupCB.select(\"div.store_txt > h2\")\n",
    "            store_name = store_name_h2[0].string\n",
    "            print(store_name)  #매장 이름 출력하기\n",
    "            store_info = soupCB.select(\"div.store_txt > table.store_table > tbody > tr > td\")\n",
    "            store_address_list = list(store_info[2])\n",
    "            store_address = store_address_list[0]\n",
    "            store_phone = store_info[3].string\n",
    "            result.append([store_name]+[store_address]+[store_phone])\n",
    "        except:\n",
    "            continue \n",
    "    return\n",
    "\n",
    "result = []\n",
    "print('CoffeeBean store crawling >>>>>>>>>>>>>>>>>>>>>>>>>>')\n",
    "CoffeeBean_store(result)  #[CODE 1]\n",
    "    \n",
    "CB_tbl = pd.DataFrame(result, columns=('store', 'address','phone'))\n",
    "CB_tbl.to_csv('output/CoffeeBean.csv', encoding='cp949', mode='w', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydatavenv",
   "language": "python",
   "name": "pydatavenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
